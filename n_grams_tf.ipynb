{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "n grams tf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-2xkFAzVfyx"
      },
      "source": [
        "\r\n",
        "https://github.com/yandexdataschool/nlp_course/blob/2020/week03_lm/homework_tf.ipynb\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98PnKVsdVVyX"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtlTHBzgVYYa"
      },
      "source": [
        "\r\n",
        "BOS, EOS = ' ', '\\n'\r\n",
        "\r\n",
        "data = pd.read_json(\"./arxivData.json\")\r\n",
        "lines = data.apply(lambda row: (row['title'] + ' ; ' + row['summary'])[:512], axis=1) \\\r\n",
        "            .apply(lambda line: BOS + line.replace(EOS, ' ') + EOS) \\\r\n",
        "            .tolist()\r\n",
        "\r\n",
        "# if you missed the seminar, download data here - https://yadi.sk/d/_nGyU2IajjR9-w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq3ojiJKVYgj"
      },
      "source": [
        "\r\n",
        "# get all unique characters from lines (including capital letters and symbols)\r\n",
        "tokens = <YOUR CODE>\r\n",
        "\r\n",
        "tokens = sorted(tokens)\r\n",
        "n_tokens = len(tokens)\r\n",
        "print ('n_tokens = ',n_tokens)\r\n",
        "assert 100 < n_tokens < 150\r\n",
        "assert BOS in tokens, EOS in tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7d9aAeFVYid"
      },
      "source": [
        "# dictionary of character -> its identifier (index in tokens list)\r\n",
        "token_to_id = <YOUR CODE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvvhQOkzVYkn"
      },
      "source": [
        "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\r\n",
        "for i in range(n_tokens):\r\n",
        "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\r\n",
        "\r\n",
        "print(\"Seems alright!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkdT91whVYoa"
      },
      "source": [
        "def to_matrix(lines, max_len=None, pad=token_to_id[EOS], dtype='int32'):\r\n",
        "    \"\"\"Casts a list of lines into tf-digestable matrix\"\"\"\r\n",
        "    max_len = max_len or max(map(len, lines))\r\n",
        "    lines_ix = np.full([len(lines), max_len], pad, dtype=dtype)\r\n",
        "    for i in range(len(lines)):\r\n",
        "        line_ix = list(map(token_to_id.get, lines[i][:max_len]))\r\n",
        "        lines_ix[i, :len(line_ix)] = line_ix\r\n",
        "    return lines_ix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAWgsUZfV9Ru"
      },
      "source": [
        "#Example: cast 4 random names to matrices, pad with zeros\r\n",
        "dummy_lines = [\r\n",
        "    ' abc\\n',\r\n",
        "    ' abacaba\\n',\r\n",
        "    ' abc1234567890\\n',\r\n",
        "]\r\n",
        "print(to_matrix(dummy_lines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlRcSYKEV9Ms"
      },
      "source": [
        "\r\n",
        "import tensorflow as tf\r\n",
        "keras, L = tf.keras, tf.keras.layers\r\n",
        "assert tf.__version__.startswith('2'), \"Current tf version: {}; required: 2.0.*\".format(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-YK5qdxV9Hz"
      },
      "source": [
        "class FixedWindowLanguageModel(L.Layer):\r\n",
        "    def __init__(self, n_tokens=n_tokens, emb_size=16, hid_size=64):\r\n",
        "        \"\"\" \r\n",
        "        A fixed window model that looks on at least 5 previous symbols.\r\n",
        "        \r\n",
        "        Note: fixed window LM is effectively performing a convolution over a sequence of words.\r\n",
        "        This convolution only looks on current and previous words.\r\n",
        "        Such convolution can be represented as a sequence of 2 operations:\r\n",
        "        - pad input vectors by {strides * (filter_size - 1)} zero vectors on the \"left\", do not pad right\r\n",
        "        - perform regular convolution with {filter_size} and {strides}\r\n",
        "        - If you're absolutely lost, here's a hint: use ZeroPadding1D and Conv1D from keras.layers\r\n",
        "        You can stack several convolutions at once\r\n",
        "        \"\"\"\r\n",
        "        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\r\n",
        "        \r\n",
        "        #YOUR CODE - create layers/variables and any metadata you want, e.g. self.emb = L.Embedding(...)\r\n",
        "        \r\n",
        "        <...>\r\n",
        "        \r\n",
        "        #END OF YOUR CODE\r\n",
        "            \r\n",
        "    def __call__(self, input_ix):\r\n",
        "        \"\"\"\r\n",
        "        compute language model logits given input tokens\r\n",
        "        :param input_ix: batch of sequences with token indices, tf tensor: int32[batch_size, sequence_length]\r\n",
        "        :returns: pre-softmax linear outputs of language model [batch_size, sequence_length, n_tokens]\r\n",
        "            these outputs will be used as logits to compute P(x_t | x_0, ..., x_{t - 1})\r\n",
        "        \"\"\"\r\n",
        "        # YOUR CODE - apply layers, see docstring above\r\n",
        "        return <...>\r\n",
        "    \r\n",
        "    def get_possible_next_tokens(self, prefix=BOS, temperature=1.0, max_len=100):\r\n",
        "        \"\"\" :returns: probabilities of next token, dict {token : prob} for all tokens \"\"\"\r\n",
        "        prefix_ix = tf.convert_to_tensor(to_matrix([prefix]), tf.int32)\r\n",
        "        probs = tf.nn.softmax(self(prefix_ix)[0, -1]).numpy()  # shape: [n_tokens]\r\n",
        "        return dict(zip(tokens, probs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WFphFmKV9C5"
      },
      "source": [
        "model = FixedWindowLanguageModel()\r\n",
        "\r\n",
        "# note: tensorflow and keras layers create variables only after they're first applied (called)\r\n",
        "dummy_input_ix = tf.constant(to_matrix(dummy_lines))\r\n",
        "dummy_logits = model(dummy_input_ix)\r\n",
        "\r\n",
        "print('Weights:', tuple(w.name for w in model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrbAvGONWPHj"
      },
      "source": [
        "assert isinstance(dummy_logits, tf.Tensor)\r\n",
        "assert dummy_logits.shape == (len(dummy_lines), max(map(len, dummy_lines)), n_tokens), \"please check output shape\"\r\n",
        "assert np.all(np.isfinite(dummy_logits)), \"inf/nan encountered\"\r\n",
        "assert not np.allclose(dummy_logits.numpy().sum(-1), 1), \"please predict linear outputs, don't use softmax (maybe you've just got unlucky)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiNdrRdFWPEM"
      },
      "source": [
        "\r\n",
        "# test for lookahead\r\n",
        "dummy_input_ix_2 = tf.constant(to_matrix([line[:3] + 'e' * (len(line) - 3) for line in dummy_lines]))\r\n",
        "dummy_logits_2 = model(dummy_input_ix_2)\r\n",
        "\r\n",
        "assert np.allclose(dummy_logits[:, :3] - dummy_logits_2[:, :3], 0), \"your model's predictions depend on FUTURE tokens. \" \\\r\n",
        "    \" Make sure you don't allow any layers to look ahead of current token.\" \\\r\n",
        "    \" You can also get this error if your model is not deterministic (e.g. dropout). Disable it for this test.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dYc8GHeWPBw"
      },
      "source": [
        "def compute_lengths(input_ix, eos_ix=token_to_id[EOS]):\r\n",
        "    \"\"\" compute length of each line in input ix (incl. first EOS), int32 vector of shape [batch_size] \"\"\"\r\n",
        "    count_eos = tf.cumsum(tf.cast(tf.equal(input_ix, eos_ix), tf.int32), axis=1, exclusive=True)\r\n",
        "    lengths = tf.reduce_sum(tf.cast(tf.equal(count_eos, 0), tf.int32), axis=1)\r\n",
        "    return lengths\r\n",
        "\r\n",
        "print('matrix:\\n', dummy_input_ix.numpy())\r\n",
        "print('lengths:', compute_lengths(dummy_input_ix).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leFNO7HJWO-p"
      },
      "source": [
        "def compute_loss(model, input_ix):\r\n",
        "    \"\"\"\r\n",
        "    :param model: language model that can compute next token logits given token indices\r\n",
        "    :param input ix: int32 matrix of tokens, shape: [batch_size, length]; padded with eos_ix\r\n",
        "    \"\"\"\r\n",
        "    input_ix = tf.convert_to_tensor(input_ix, dtype=tf.int32)\r\n",
        "    \r\n",
        "\r\n",
        "    logits = model(input_ix[:, :-1])\r\n",
        "    reference_answers = input_ix[:, 1:]\r\n",
        "\r\n",
        "    # Your task: implement loss function as per formula above\r\n",
        "    # your loss should only be computed on actual tokens, excluding padding\r\n",
        "    # predicting actual tokens and first EOS do count. Subsequent EOS-es don't\r\n",
        "    # you will likely need to use compute_lengths and/or tf.sequence_mask to get it right.\r\n",
        "\r\n",
        "    <YOUR CODE>\r\n",
        "\r\n",
        "    return <YOUR CODE: return scalar loss>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU3xbNJWWhIN"
      },
      "source": [
        "loss_1 = compute_loss(model, to_matrix(dummy_lines, max_len=15))\r\n",
        "loss_2 = compute_loss(model, to_matrix(dummy_lines, max_len=16))\r\n",
        "assert (np.ndim(loss_1) == 0) and (0 < loss_1 < 100), \"loss must be a positive scalar\"\r\n",
        "assert np.allclose(loss_1, loss_2), 'do not include  AFTER first EOS into loss. '\\\r\n",
        "    'Hint: use tf.sequence_mask. Beware +/-1 errors. And be careful when averaging!'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Hq_P33WhEk"
      },
      "source": [
        "def score_lines(model, dev_lines, batch_size):\r\n",
        "    \"\"\" computes average loss over the entire dataset \"\"\"\r\n",
        "    dev_loss_num, dev_loss_len = 0., 0.\r\n",
        "    for i in range(0, len(dev_lines), batch_size):\r\n",
        "        batch_ix = to_matrix(dev_lines[i: i + batch_size])\r\n",
        "        dev_loss_num += compute_loss(model, batch_ix) * len(batch_ix)\r\n",
        "        dev_loss_len += len(batch_ix)\r\n",
        "    return dev_loss_num / dev_loss_len\r\n",
        "\r\n",
        "def generate(model, prefix=BOS, temperature=1.0, max_len=100):\r\n",
        "    \"\"\"\r\n",
        "    Samples output sequence from probability distribution obtained by model\r\n",
        "    :param temperature: samples proportionally to model probabilities ^ temperature\r\n",
        "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\r\n",
        "    \"\"\"\r\n",
        "    while True:\r\n",
        "        token_probs = model.get_possible_next_tokens(prefix)\r\n",
        "        tokens, probs = zip(*token_probs.items())\r\n",
        "        if temperature == 0:\r\n",
        "            next_token = tokens[np.argmax(probs)]\r\n",
        "        else:\r\n",
        "            probs = np.array([p ** (1. / temperature) for p in probs])\r\n",
        "            probs /= sum(probs)\r\n",
        "            next_token = np.random.choice(tokens, p=probs)\r\n",
        "        \r\n",
        "        prefix += next_token\r\n",
        "        if next_token == EOS or len(prefix) > max_len: break\r\n",
        "    return prefix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmltE1XsWhCg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kQlB9GnWg7t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}